{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from keras import layers\n",
    "# from keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.__version__\n",
    "tf.random.set_seed(123)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data paths\n",
    "parent_dir = r'/mnt/E/datasets/pneumonia_kaggle'\n",
    "train_dir = os.path.join(parent_dir,'train')\n",
    "test_dir = os.path.join(parent_dir,'test')\n",
    "val_dir = os.path.join(parent_dir,'val')\n",
    "save_dir = r'/mnt/D/Projects/Pneumonia Kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "batch_size = 32\n",
    "img_height = 450\n",
    "img_width = 450\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Converting data directory to a training set\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    shuffle=True,\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    label_mode='binary',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Converting data directory to a validation set\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    shuffle=True,\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    label_mode='binary',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Converting data directory to a test set\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    shuffle=True,\n",
    "    labels='inferred',\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    label_mode='binary',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "# Getting class names\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0/255)\n",
    "train_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_ds = val_ds.map(lambda image,label:(rescale(image),label))\n",
    "test_ds  = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalizing colors\n",
    "# normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "# normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "# image_batch, labels_batch = next(iter(normalized_ds))\n",
    "# first_image = image_batch[0]\n",
    "# print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structuring the model\n",
    "num_classes = len(class_names)\n",
    "\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=1, verbose=1, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(f\"{save_dir}/pneumonia_{epochs}e_binary_gray.h5\")]\n",
    "model = keras.models.Sequential()\n",
    "# model.add(layers.experimental.preprocessing.Rescaling(1./255))\n",
    "model.add(layers.Conv2D(32,3, activation='relu', input_shape=(img_width,img_height,1)))\n",
    "model.add(layers.MaxPool2D())\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32,3,activation='relu'))\n",
    "model.add(layers.MaxPool2D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.add(layers.Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "              optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "163/163 [==============================] - 980s 6s/step - loss: 0.2778 - accuracy: 0.9153 - val_loss: 0.1055 - val_accuracy: 0.9375\n",
      "Epoch 2/12\n",
      "163/163 [==============================] - 968s 6s/step - loss: 0.0609 - accuracy: 0.9780 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 3/12\n",
      "163/163 [==============================] - 950s 6s/step - loss: 0.0447 - accuracy: 0.9837 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 4/12\n",
      "163/163 [==============================] - 948s 6s/step - loss: 0.0315 - accuracy: 0.9889 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
      "Epoch 5/12\n",
      "163/163 [==============================] - 956s 6s/step - loss: 0.0170 - accuracy: 0.9941 - val_loss: 0.1703 - val_accuracy: 0.9375\n",
      "Epoch 6/12\n",
      "163/163 [==============================] - 945s 6s/step - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 7/12\n",
      "163/163 [==============================] - 942s 6s/step - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 8/12\n",
      "163/163 [==============================] - 934s 6s/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.3287 - val_accuracy: 0.8750\n",
      "Epoch 9/12\n",
      "163/163 [==============================] - 936s 6s/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0520 - val_accuracy: 0.9375\n",
      "Epoch 10/12\n",
      "163/163 [==============================] - 934s 6s/step - loss: 7.0729e-04 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9375\n",
      "Epoch 11/12\n",
      "163/163 [==============================] - ETA: 0s - loss: 1.9043e-04 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 10.\n",
      "163/163 [==============================] - 936s 6s/step - loss: 1.9043e-04 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 448, 448, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 224, 224, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 222, 222, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 394272)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                25233472  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,243,105\n",
      "Trainable params: 25,243,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model performance\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs_range = range(epochs)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "# plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(epochs_range, loss, label='Training Loss')\n",
    "# plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# model.save(f\"{save_dir}/pneumonia_{epochs}e_binary_gray.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(f\"{save_dir}/pneumonia_{epochs}e_binary_gray.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing test colors\n",
    "# normalization_test_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "# normalized_test_ds = test_ds.map(lambda x, y: (normalization_test_layer(x), y))\n",
    "# image_test_batch, labels_test_batch = next(iter(normalized_test_ds))\n",
    "# first_test_image = image_test_batch[0]\n",
    "# print(np.min(first_test_image), np.max(first_test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 875ms/step\n"
     ]
    }
   ],
   "source": [
    "# Getting x and y from the test set\n",
    "test_preds = []\n",
    "y_test = []\n",
    "for x,y in test_ds:\n",
    "    test_pred = model.predict(x)\n",
    "    # x_test.append(x)\n",
    "    # y_test.append(y)\n",
    "    test_preds.extend(test_pred)\n",
    "    y_test.extend(y)\n",
    "\n",
    "preds = np.array(test_preds)\n",
    "y_pred = np.where(preds > 0.5, 1, 0)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of the model is 1.00\n",
      "Precision of the model is 0.70\n",
      "F1-score of the model is 0.82\n"
     ]
    }
   ],
   "source": [
    "# Assessing model performance using confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm  = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2*((precision * recall)/(precision + recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-score of the model is {:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 450, 1)\n",
      "(1, 450, 450, 1)\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "[1.]\n",
      "[1]\n",
      "This image is 0.00 percent normal and 100.00 percent pneumonia.\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# E:\\Downloads\\chest pneumonia.jpg\n",
    "img = keras.preprocessing.image.load_img(r\"/mnt/E/datasets/pneumonia_kaggle/train/PNEUMONIA/person3_bacteria_10.jpeg\",\n",
    "                                         target_size=(img_width, img_height),\n",
    "                                         color_mode='grayscale')\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "print(img_array.shape)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "print(img_array.shape)\n",
    "predictions = model.predict(img_array)\n",
    "score = predictions[0]\n",
    "print(score)\n",
    "tmp_pred = np.where(score > 0.5, 1, 0)\n",
    "print(tmp_pred)\n",
    "print(\"This image is %.2f percent normal and %.2f percent pneumonia.\" % (100 * (1 - score), 100 * score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame(columns=['File', 'Category', 'Prediction','Class'])\n",
    "# class_names = os.listdir(test_dir)\n",
    "# count = 0\n",
    "# # Access test files \n",
    "# for c in class_names:\n",
    "#     path = os.path.join(test_dir,c)\n",
    "#     print(path)\n",
    "#     files = os.listdir(path)\n",
    "#     for file in files:\n",
    "#         # Load and preprocess image\n",
    "#         img_path = os.path.join(path,file)\n",
    "#         img = keras.preprocessing.image.load_img(img_path,\n",
    "#                                          target_size=(img_width, img_height),\n",
    "#                                          color_mode='grayscale')\n",
    "#         img_array = keras.preprocessing.image.img_to_array(img)\n",
    "#         # print(img_array.shape)\n",
    "#         img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "#         # print(img_array.shape)\n",
    "#         predictions = model.predict(img_array)\n",
    "#         score = predictions[0]\n",
    "#         # Set threshold for classes\n",
    "#         img_pred = np.where(score > 0.5, 1, 0)\n",
    "\n",
    "#         # Write rows\n",
    "#         df.at[count, 'File'] = file\n",
    "#         df.at[count, 'Category'] = c\n",
    "#         df.at[count, 'Prediction'] = score\n",
    "#         df.at[count, 'Class'] = img_pred\n",
    "#         count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(f\"{parent_dir}/test_preds8e.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the model overfits the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
